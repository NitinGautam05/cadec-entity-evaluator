{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbff5e3-b8c6-4be6-a4d6-04b33708d781",
   "metadata": {},
   "source": [
    "# Task 4: ADR-Only Evaluation (Text-Based) using MedDRA Ground Truth\n",
    "\n",
    "This script evaluates the predicted annotations for ADR (Adverse Drug Reaction) mentions by comparing them against the MedDRA-standardized ground truth. It uses text-based normalization and standard precision, recall, and F1-score metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8fb45d-940c-4038-9631-d6f7713713eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Predicted File: ARTHROTEC.104.ann\n",
      "ðŸ“„ MedDRA File:    ARTHROTEC.104.ann\n",
      "\n",
      "âœ… Ground Truth ADR Texts:\n",
      "  â€¢ constipation\n",
      "  â€¢ diarrhea\n",
      "  â€¢ fatigue\n",
      "\n",
      "ðŸ”® Predicted ADR Texts:\n",
      "  â€¢ constipation\n",
      "  â€¢ severe osteoarthritis\n",
      "  â€¢ some diarrhea\n",
      "\n",
      "ðŸ“Š Text-Based Evaluation Metrics:\n",
      "Precision: 0.3333\n",
      "Recall:    0.3333\n",
      "F1-score:  0.3333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, extra spaces.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text.lower()).strip()\n",
    "\n",
    "def load_adr_texts(file_path, is_meddra=False):\n",
    "    \"\"\"Extract ADR mention texts from predicted or MedDRA file.\"\"\"\n",
    "    adr_texts = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "\n",
    "            if is_meddra:\n",
    "                try:\n",
    "                    text = parts[-1]\n",
    "                    text = normalize_text(text)\n",
    "                    if text:\n",
    "                        adr_texts.add(text)\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                label_info = parts[1].split()\n",
    "                label = label_info[0]\n",
    "                if label != 'ADR':\n",
    "                    continue\n",
    "                try:\n",
    "                    text = parts[-1]\n",
    "                    text = normalize_text(text)\n",
    "                    if text:\n",
    "                        adr_texts.add(text)\n",
    "                except:\n",
    "                    continue\n",
    "    return adr_texts\n",
    "\n",
    "def evaluate_adr_text_only(pred_file, meddra_file):\n",
    "    \"\"\"Text-based evaluation of ADR mentions.\"\"\"\n",
    "    print(f\"\\nðŸ“„ Predicted File: {os.path.basename(pred_file)}\")\n",
    "    print(f\"ðŸ“„ MedDRA File:    {os.path.basename(meddra_file)}\")\n",
    "\n",
    "    pred_texts = load_adr_texts(pred_file, is_meddra=False)\n",
    "    true_texts = load_adr_texts(meddra_file, is_meddra=True)\n",
    "\n",
    "    print(\"\\nâœ… Ground Truth ADR Texts:\")\n",
    "    for t in sorted(true_texts): print(\"  â€¢\", t)\n",
    "\n",
    "    print(\"\\nðŸ”® Predicted ADR Texts:\")\n",
    "    for t in sorted(pred_texts): print(\"  â€¢\", t)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for pred in pred_texts:\n",
    "        if pred in true_texts:\n",
    "            y_pred.append(1)\n",
    "            y_true.append(1)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "            y_true.append(0)\n",
    "\n",
    "    for truth in true_texts:\n",
    "        if truth not in pred_texts:\n",
    "            y_pred.append(0)\n",
    "            y_true.append(1)\n",
    "\n",
    "    if not y_true:\n",
    "        print(\"\\nâš ï¸  No ADR labels found in ground truth. Cannot evaluate.\")\n",
    "        return\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "\n",
    "    print(\"\\nðŸ“Š Text-Based Evaluation Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "predicted_path = \"./predicted/ARTHROTEC.104.ann\"\n",
    "meddra_path = r\"./cadec/meddra/ARTHROTEC.104.ann\"\n",
    "\n",
    "evaluate_adr_text_only(predicted_path, meddra_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4d828-b113-4411-96a0-48fe541fffde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
