{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4ba888-a9e5-4dc5-bb04-dbef07dc0940",
   "metadata": {},
   "source": [
    "**Task5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88d60538-a670-4a53-a119-e8ef0a7f9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating 5 random files from ./predicted\n",
      "\n",
      "üìÑ Evaluating: ARTHROTEC.105.ann\n",
      "Metrics for ARTHROTEC.105.ann:\n",
      "  True Positives: 2\n",
      "  False Positives: 4\n",
      "  False Negatives: 9\n",
      "  Precision: 0.3333\n",
      "  Recall: 0.1818\n",
      "  F1 Score: 0.2353\n",
      "\n",
      "üìÑ Evaluating: ARTHROTEC.64.ann\n",
      "Metrics for ARTHROTEC.64.ann:\n",
      "  True Positives: 2\n",
      "  False Positives: 6\n",
      "  False Negatives: 7\n",
      "  Precision: 0.25\n",
      "  Recall: 0.2222\n",
      "  F1 Score: 0.2353\n",
      "\n",
      "üìÑ Evaluating: LIPITOR.844.ann\n",
      "Metrics for LIPITOR.844.ann:\n",
      "  True Positives: 1\n",
      "  False Positives: 7\n",
      "  False Negatives: 5\n",
      "  Precision: 0.125\n",
      "  Recall: 0.1667\n",
      "  F1 Score: 0.1429\n",
      "\n",
      "üìÑ Evaluating: LIPITOR.846.ann\n",
      "Metrics for LIPITOR.846.ann:\n",
      "  True Positives: 0\n",
      "  False Positives: 4\n",
      "  False Negatives: 2\n",
      "  Precision: 0.0\n",
      "  Recall: 0.0\n",
      "  F1 Score: 0.0\n",
      "\n",
      "üìÑ Evaluating: ARTHROTEC.100.ann\n",
      "Metrics for ARTHROTEC.100.ann:\n",
      "  True Positives: 1\n",
      "  False Positives: 7\n",
      "  False Negatives: 4\n",
      "  Precision: 0.125\n",
      "  Recall: 0.2\n",
      "  F1 Score: 0.1538\n",
      "\n",
      "üìà Average Metrics Across Files:\n",
      "  True Positives: 1.2\n",
      "  False Positives: 5.6\n",
      "  False Negatives: 5.4\n",
      "  Precision: 0.1667\n",
      "  Recall: 0.1541\n",
      "  F1 Score: 0.1535\n",
      "\n",
      "‚úÖ Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------------\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ------------------------\n",
    "ground_truth_dir = \"./cadec/original\"\n",
    "predicted_dir = \"./predicted\"\n",
    "num_files_to_evaluate = 5\n",
    "\n",
    "# ------------------------\n",
    "# üîç LOAD ANN FILES\n",
    "# ------------------------\n",
    "def load_ann_file(filepath):\n",
    "    entries = set()\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\") or not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            tag_id, tag_info, phrase = parts\n",
    "            tag_parts = tag_info.split()\n",
    "            label = tag_parts[0]\n",
    "            start = int(tag_parts[1])\n",
    "            end = int(tag_parts[-1])\n",
    "            entries.add((start, end, label))\n",
    "    return entries\n",
    "\n",
    "# ------------------------\n",
    "# üßÆ COMPUTE METRICS\n",
    "# ------------------------\n",
    "def evaluate_ann(predicted, ground_truth):\n",
    "    tp = len(predicted & ground_truth)\n",
    "    fp = len(predicted - ground_truth)\n",
    "    fn = len(ground_truth - predicted)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"True Positives\": tp,\n",
    "        \"False Positives\": fp,\n",
    "        \"False Negatives\": fn,\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1 Score\": round(f1, 4)\n",
    "    }\n",
    "\n",
    "# ------------------------\n",
    "# ‚úÖ RUN EVALUATION ON RANDOM FILES\n",
    "# ------------------------\n",
    "def evaluate_directory(ground_truth_dir, predicted_dir, num_files):\n",
    "    # Get all .ann files in the predicted directory\n",
    "    ann_files = [f for f in os.listdir(predicted_dir) if f.endswith(\".ann\")]\n",
    "    \n",
    "    # Select random subset of files\n",
    "    if len(ann_files) > num_files:\n",
    "        ann_files = random.sample(ann_files, num_files)\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    print(f\"\\nüìä Evaluating {num_files} random files from {predicted_dir}\")\n",
    "    \n",
    "    for file_name in ann_files:\n",
    "        gt_path = os.path.join(ground_truth_dir, file_name)\n",
    "        pred_path = os.path.join(predicted_dir, file_name)\n",
    "        \n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"‚ùå Ground truth file not found for {file_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(pred_path):\n",
    "            print(f\"‚ùå Predicted file not found for {file_name}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüìÑ Evaluating: {file_name}\")\n",
    "        gt_spans = load_ann_file(gt_path)\n",
    "        pred_spans = load_ann_file(pred_path)\n",
    "        \n",
    "        metrics = evaluate_ann(pred_spans, gt_spans)\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"Metrics for {file_name}:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Compute average metrics\n",
    "    if all_metrics:\n",
    "        avg_metrics = {\n",
    "            \"True Positives\": sum(m[\"True Positives\"] for m in all_metrics) / len(all_metrics),\n",
    "            \"False Positives\": sum(m[\"False Positives\"] for m in all_metrics) / len(all_metrics),\n",
    "            \"False Negatives\": sum(m[\"False Negatives\"] for m in all_metrics) / len(all_metrics),\n",
    "            \"Precision\": round(sum(m[\"Precision\"] for m in all_metrics) / len(all_metrics), 4),\n",
    "            \"Recall\": round(sum(m[\"Recall\"] for m in all_metrics) / len(all_metrics), 4),\n",
    "            \"F1 Score\": round(sum(m[\"F1 Score\"] for m in all_metrics) / len(all_metrics), 4)\n",
    "        }\n",
    "        \n",
    "        print(\"\\nüìà Average Metrics Across Files:\")\n",
    "        for k, v in avg_metrics.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Evaluation complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(ground_truth_dir) or not os.path.exists(predicted_dir):\n",
    "        raise FileNotFoundError(\"Check that both ground truth and predicted directories exist.\")\n",
    "    \n",
    "    evaluate_directory(ground_truth_dir, predicted_dir, num_files_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936b5f0-04c8-4b72-8a1b-170298ea00cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
